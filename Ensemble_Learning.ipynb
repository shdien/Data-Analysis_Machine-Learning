{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.  What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "1. Key Idea Behind Ensemble Learning:-\n",
        "\n",
        "“Many weak or diverse learners together form a strong learner.”\n",
        "\n",
        "Instead of relying on one model, ensemble learning aggregates predictions from multiple models to:\n",
        "\n",
        "Reduce errors\n",
        "\n",
        "Improve generalization\n",
        "\n",
        "Increase stability and accuracy\n",
        "\n",
        "2. Why Ensemble Learning Works:-\n",
        "\n",
        "Ensembles improve performance by addressing three main types of errors:\n",
        "\n",
        "Bias → Error from overly simple models\n",
        "\n",
        "Variance → Error from overly complex, unstable models\n",
        "\n",
        "Noise → Random fluctuations in data\n",
        "\n",
        "By combining models, ensembles balance bias and variance more effectively.\n",
        "\n",
        "3. How Models Are Combined:-\n",
        "\n",
        "Classification → Majority voting / weighted voting\n",
        "\n",
        "Regression → Averaging predictions\n",
        "\n",
        "4. Advantages of Ensemble Learning:-\n",
        "\n",
        "Higher accuracy\n",
        "\n",
        "Better generalization\n",
        "\n",
        "More robust to overfitting\n",
        "\n",
        "Works well with complex datasets\n",
        "\n",
        "5. Limitations\n",
        "\n",
        "Increased computational cost\n",
        "\n",
        "Less interpretability\n",
        "\n",
        "More complex to implement and tune"
      ],
      "metadata": {
        "id": "Lg9aNgE_Y1Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "| Aspect          | Bagging (Bootstrap Aggregating)      | Boosting                                  |\n",
        "| --------------- | ------------------------------------ | ----------------------------------------- |\n",
        "| Training Style  | Parallel, independent models         | Sequential, dependent models              |\n",
        "| Data Sampling   | Random sampling **with replacement** | Re-weighting data (focus on hard samples) |\n",
        "| Focus           | Reduces **variance**                 | Reduces **bias**                          |\n",
        "| Handling Errors | All samples treated equally          | Misclassified samples get higher weight   |\n",
        "| Overfitting     | Helps prevent overfitting            | Can overfit if noisy data                 |\n",
        "| Model Weighting | Equal weight to all models           | Weighted models                           |\n",
        "| Speed           | Faster (parallelizable)              | Slower (sequential)                       |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jU17TjSmZgce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "1. What is Bootstrap Sampling:-\n",
        "\n",
        "From a dataset of size N, we draw N samples with replacement\n",
        "\n",
        "Some original samples may appear multiple times\n",
        "\n",
        "Some samples may not appear at all\n",
        "\n",
        "2. Role of Bootstrap Sampling in Bagging:-\n",
        "\n",
        "Bagging = Bootstrap + Aggregation\n",
        "\n",
        "How it Works:\n",
        "\n",
        "Generate multiple bootstrap samples from the original dataset\n",
        "\n",
        "Train a separate model (e.g., decision tree) on each sample\n",
        "\n",
        "Combine predictions:\n",
        "\n",
        "Classification → Majority voting\n",
        "\n",
        "Regression → Averaging\n",
        "\n",
        "3. Bootstrap Sampling in Random Forest:-\n",
        "\n",
        "Random Forest uses two sources of randomness:\n",
        "\n",
        "Bootstrap sampling of data\n",
        "\n",
        "Random subset of features at each split\n",
        "\n",
        "This double randomness:\n",
        "\n",
        "Increases model diversity\n",
        "\n",
        "Reduces correlation between trees\n",
        "\n",
        "Improves generalization\n",
        "\n",
        "4. Why Bootstrap Sampling Improves Performance:-\n",
        "\n",
        "Reduces variance of unstable models (like decision trees)\n",
        "\n",
        "Makes models more robust to noise\n",
        "\n",
        "Prevents overfitting\n",
        "\n",
        "5. Simple Example\n",
        "\n",
        "Original data:\n",
        "[A, B, C, D, E]\n",
        "\n",
        "Bootstrap sample:\n",
        "[B, C, C, E, A]\n",
        "\n",
        "(D is out-of-bag)"
      ],
      "metadata": {
        "id": "6t7OzMXrZyGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "1. What are Out-of-Bag (OOB) Samples:-\n",
        "\n",
        "In bootstrap sampling:\n",
        "\n",
        "From a dataset of size N, we sample N points with replacement\n",
        "\n",
        "About 63.2% of unique samples are selected\n",
        "\n",
        "The remaining ~36.8% are Out-of-Bag samples\n",
        "\n",
        "These OOB samples were never seen by a given tree during training.\n",
        "\n",
        "2. How OOB Samples are Used:-\n",
        "\n",
        "For each data point:\n",
        "\n",
        "Identify all trees where this point was OOB\n",
        "\n",
        "Use those trees to make a prediction for that point\n",
        "\n",
        "Aggregate predictions:\n",
        "\n",
        "Classification → Majority vote\n",
        "\n",
        "Regression → Average prediction\n",
        "\n",
        "3. What is OOB Score:-\n",
        "\n",
        "The OOB score is the overall accuracy (classification) or R² / MSE (regression) computed using OOB predictions for all samples.\n",
        "\n",
        "OOB Score =\n",
        "Correct OOB predictions\n",
        "Total samples\n",
        "OOB Score=\n",
        "Total samples\n",
        "Correct OOB predictions\n",
        "\n",
        "4. Why OOB Score is Useful:-\n",
        "\n",
        "No need for a separate validation or test set\n",
        "\n",
        "Efficient use of data\n",
        "\n",
        "Provides an unbiased estimate of generalization performance\n",
        "\n",
        "Especially useful when data is limited\n",
        "\n",
        "\n",
        "\t​\n"
      ],
      "metadata": {
        "id": "FwgfMPIzbem4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "| Aspect              | Single Decision Tree            | Random Forest        |\n",
        "| ------------------- | ------------------------------- | -------------------- |\n",
        "| Number of Models    | One tree                        | Many trees           |\n",
        "| Stability           | High variance, unstable         | Low variance, stable |\n",
        "| Sensitivity to Data | Very sensitive to small changes | Much less sensitive  |\n",
        "| Overfitting         | High risk                       | Reduced risk         |\n",
        "| Feature Bias        | Can strongly favor one feature  | Bias averaged out    |\n",
        "| Reliability         | Lower                           | Higher               |\n",
        "| Interpretability    | Very interpretable              | Less interpretable   |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rQdzo9YvdtNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to:\n",
        "#   Load the Breast Cancer dataset using\n",
        "#   sklearn.datasets.load_breast_cancer()\n",
        "#   Train a Random Forest Classifier\n",
        "#   Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame for easy viewing\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort features by importance (descending)\n",
        "top_5_features = feature_importance_df.sort_values(\n",
        "    by='Importance', ascending=False\n",
        ").head(5)\n",
        "\n",
        "# Print top 5 important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzN4SyTzeBxX",
        "outputId": "7bf71759-b805-4a01-8316-0cb68dd5817b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to:\n",
        "#   Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "#   Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Train Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvDu5zizM1SZ",
        "outputId": "108eded8-0703-4c37-f109-5ed46d78268f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.  Write a Python program to:\n",
        "#    Train a Random Forest Classifier\n",
        "#    Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "#    Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit GridSearch\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Final accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Final Test Accuracy:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83YbIKJuNcAT",
        "outputId": "e745aa83-8bbe-4f8a-ba39-f6c466fa7c17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Test Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to:\n",
        "#   Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "#   Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Bagging Regressor\n",
        "# -----------------------------\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "\n",
        "# -----------------------------\n",
        "# Random Forest Regressor\n",
        "# -----------------------------\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h0jdgGTOE3m",
        "outputId": "3dc3e252-6e6d-4429-cced-477417c174e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2568358813508342\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to: ● Choose between Bagging or Boosting ● Handle overfitting ● Select base models ● Evaluate performance using cross-validation ● Justify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "1. Choosing Between Bagging and Boosting:-\n",
        "\n",
        "Step 1: Understand the data & business risk:\n",
        "\n",
        "Loan default data is usually:\n",
        "\n",
        "Imbalanced (fewer defaults than non-defaults)\n",
        "\n",
        "Noisy (missing values, reporting errors)\n",
        "\n",
        "High-stakes (false negatives are costly\n",
        "\n",
        "Decision:-\n",
        "\n",
        "Start with Bagging (e.g., Random Forest)\n",
        "\n",
        "Robust to noise\n",
        "\n",
        "Reduces variance\n",
        "\n",
        "Then try Boosting (e.g., Gradient Boosting / XGBoost)\n",
        "\n",
        "Focuses on hard-to-classify defaulters\n",
        "\n",
        "Often delivers higher predictive power\n",
        "\n",
        "2. Handling Overfitting (Critical in Finance):-\n",
        "\n",
        "Train-test split with time awareness:-\n",
        "\n",
        "Ensure no data leakage (use past data to predict future defaults)\n",
        "\n",
        "Regularization:-\n",
        "\n",
        "Limit tree depth (max_depth)\n",
        "\n",
        "Use min_samples_leaf\n",
        "\n",
        "Ensemble controls:-\n",
        "\n",
        "Bagging → More trees, shallower depth\n",
        "\n",
        "Boosting → Lower learning rate, early stopping\n",
        "\n",
        "Feature selection:-\n",
        "\n",
        "Remove highly correlated or unstable features\n",
        "\n",
        "Out-of-Bag (OOB) / Cross-validation\n",
        "\n",
        "Continuous performance monitoring\n",
        "\n",
        "3. Selecting Base Models:-\n",
        "\n",
        "Why decision trees:-\n",
        "\n",
        "Handle non-linear relationships\n",
        "\n",
        "Work with mixed data types\n",
        "\n",
        "Capture interaction between demographic & transaction features\n",
        "\n",
        "Base model choices:-\n",
        "\n",
        "Bagging:\n",
        "\n",
        "Decision Trees (high variance → bagging benefits)\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Shallow decision trees (decision stumps)\n",
        "\n",
        "4. Evaluating Performance Using Cross-Validation:-\n",
        "\n",
        "Use Stratified K-Fold CV:-\n",
        "\n",
        "Maintains default / non-default ratio\n",
        "\n",
        "Evaluate using business-relevant metrics:-\n",
        "\n",
        "AUC-ROC → Ranking risk\n",
        "\n",
        "Precision-Recall → Default detection\n",
        "\n",
        "F1-score → Balance errors\n",
        "\n",
        "Compare:-\n",
        "\n",
        "Single model vs Bagging vs Boosting\n",
        "\n",
        "Check stability across folds:-\n",
        "\n",
        "Consistent performance = trustworthy model\n",
        "\n",
        "5. How Ensemble Learning Improves Decision-Making:-\n",
        "\n",
        "Business Impact:-\n",
        "\n",
        "More accurate risk assessment\n",
        "\n",
        "Reduced false negatives (missed defaulters)\n",
        "\n",
        "Better loan pricing and approval decisions\n",
        "\n",
        "Technical Benefits:-\n",
        "\n",
        "Combines multiple perspectives of data\n",
        "\n",
        "Reduces model bias and variance\n",
        "\n",
        "Handles complex customer behavior patterns"
      ],
      "metadata": {
        "id": "qXfXMSJERN-V"
      }
    }
  ]
}