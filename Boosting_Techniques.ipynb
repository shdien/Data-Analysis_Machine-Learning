{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "A weak learner is a model that is only slightly better than random guessing. In Boosting, the most common weak learner is a Decision Stump‚Äîa decision tree with only one split (one node and two leaves).\n",
        "\n",
        "Boosting works by focusing on the \"hard\" cases.\n",
        "Here is the step-by-step logic:\n",
        "\n",
        "Initial Training: You train a weak learner (Model 1) on the entire dataset. It will get some predictions right and some wrong.\n",
        "\n",
        "Assigning Weights: The algorithm looks at the data points that Model 1 got wrong. It increases the \"weight\" or importance of those specific points.\n",
        "\n",
        "Sequential Correction: The next weak learner (Model 2) is trained.7 Because of the weighted data, Model 2 is \"forced\" to pay more attention to the mistakes made by Model.\n",
        "\n",
        "Repeat: This continues for $N$ iterations. Each subsequent model tries to solve the \"residual\" error left behind by the previous ones.\n",
        "\n",
        "Final Combined Vote: To make a final prediction, the algorithm combines the predictions of all the weak learners.10 Models that had higher accuracy are typically given more \"say\" or weight in the final decision\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TRRnWnlEUM2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "| Aspect              | AdaBoost                      | Gradient Boosting         |\n",
        "| ------------------- | ----------------------------- | ------------------------- |\n",
        "| Error focus         | Misclassified samples         | Residual errors           |\n",
        "| Data handling       | Re-weights data points        | Keeps data fixed          |\n",
        "| Optimization view   | Heuristic-based               | Gradient descent on loss  |\n",
        "| Loss function       | Exponential loss (implicitly) | Any differentiable loss   |\n",
        "| Robustness to noise | Sensitive to outliers         | More robust (with tuning) |\n"
      ],
      "metadata": {
        "id": "KK6K0uZQq4yH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. How does regularization help in XGBoost?\n",
        "\n",
        ". The Regularized Objective Function4In XGBoost, every time a new tree is added, the algorithm tries to minimize an objective function that consists of two parts:\n",
        "$Objective = \\text{Training Loss} + \\text{Regularization}\n",
        "\n",
        "Training Loss: Measures how well the model fits the training data (e.g., Mean Squared Error or Log Loss).\n",
        "\n",
        "Regularization (7$\\Omega$): Measures how complex the trees are.8 If the trees get too deep or the leaf weights get too large, this term increases, \"punishing\" the model.\n",
        "\n",
        "How Regularization Helps:-\n",
        "1. Controls Tree Complexity\n",
        "\n",
        "Œ≥ (gamma) discourages creating unnecessary splits.\n",
        "\n",
        "A split is made only if it reduces loss more than Œ≥.\n",
        "\n",
        "Leads to shallower, simpler trees.\n",
        "\n",
        "2Ô∏è. Shrinks Leaf Weights\n",
        "\n",
        "Œª (L2) and Œ± (L1) regularize leaf weights.\n",
        "\n",
        "Prevents extreme prediction values.\n",
        "\n",
        "Improves generalization.\n",
        "\n",
        "| Parameter          | Type       | Effect                        |\n",
        "| ------------------ | ---------- | ----------------------------- |\n",
        "| `gamma`            | Structural | Penalizes number of leaves    |\n",
        "| `lambda`           | L2         | Shrinks leaf weights          |\n",
        "| `alpha`            | L1         | Encourages sparsity           |\n",
        "| `max_depth`        | Structural | Limits tree depth             |\n",
        "| `min_child_weight` | Structural | Requires minimum data in leaf |\n",
        "| `subsample`        | Sampling   | Reduces variance              |\n",
        "| `colsample_bytree` | Sampling   | Reduces feature dependency    |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sSUDHul6rGgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Native Categorical Encoding (No One-Hot Explosion):-\n",
        "\n",
        "Instead of one-hot encoding, CatBoost uses target-based statistics:\n",
        "\n",
        "Encoded value=E(y‚à£category)\n",
        "\n",
        "Ordered Target Encoding (Prevents Target Leakage):-\n",
        "\n",
        "A key innovation of CatBoost is ordered boosting:\n",
        "\n",
        "Data is processed in a random order\n",
        "\n",
        "For each row, category statistics are computed using only previous rows\n",
        "\n",
        "The current target value is never used in its own encoding\n",
        "\n",
        "Special Handling of Rare Categories:-\n",
        "\n",
        "Rare or unseen categories are handled using prior statistics\n",
        "\n",
        "Smoothing reduces overfitting for low-frequency categories\n",
        "\n",
        "Ensures stable predictions on new data\n",
        "\n",
        "Categorical Feature Combinations:-\n",
        "\n",
        "CatBoost automatically creates combinations of categorical features (e.g., City √ó Product):\n",
        "\n",
        "Captures complex interactions\n",
        "\n",
        "Done in a controlled, regularized manner\n",
        "\n",
        "No manual feature engineering required\n",
        "\n"
      ],
      "metadata": {
        "id": "gERIRXXVuJFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "1. Credit Risk & Loan Default Prediction\n",
        "\n",
        "\n",
        "Captures subtle, nonlinear relationships\n",
        "\n",
        "Focuses on hard-to-classify borrowers\n",
        "\n",
        "Handles class imbalance well\n",
        "\n",
        "2. Fraud Detection\n",
        "\n",
        "\n",
        "Rare fraud cases ‚Üí boosting emphasizes misclassified samples\n",
        "\n",
        "High precision required\n",
        "\n",
        "Works well with noisy, tabular transaction data\n",
        "\n",
        "3. Online Advertising & CTR Prediction\n",
        "\n",
        "\n",
        "Extremely high-dimensional data\n",
        "\n",
        "Complex feature interactions\n",
        "\n",
        "Need incremental performance gains\n",
        "\n",
        "4. Search Ranking & Recommendation Systems\n",
        "\n",
        "\n",
        "Optimizes custom ranking loss functions\n",
        "\n",
        "Learns fine-grained feature interactions\n",
        "\n",
        "Produces strong ranking performance\n",
        "\n",
        "5. Customer Churn Prediction\n",
        "\n",
        "\n",
        "Identifies small but important churn signals\n",
        "\n",
        "Performs well on structured business data\n",
        "\n",
        "High interpretability with SHAP\n",
        "\n",
        "6. Medical Diagnosis & Risk Scoring\n",
        "\n",
        "\n",
        "High accuracy is critical\n",
        "\n",
        "Handles mixed numerical + categorical data\n",
        "\n",
        "Strong performance on tabular clinical datasets\n",
        "\n",
        "7. Demand Forecasting & Sales Prediction\n",
        "\n",
        "\n",
        "Models nonlinear trends and seasonality\n",
        "\n",
        "Robust to missing data\n",
        "\n",
        "Works well with external features (promotions, weather)"
      ],
      "metadata": {
        "id": "hS59STvUuv_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6.  Write a Python program to:\n",
        "#    Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "#    Print the model accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the AdaBoost Classifier\n",
        "model = AdaBoostClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSHyQFPovyr9",
        "outputId": "51c058f7-e76e-4efd-8125-3b694653b931"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to:\n",
        "#   Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "#   Evaluate performance using R-squared score\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NOgo0WiwP9v",
        "outputId": "9ff40e1f-f12a-4412-c302-b6e489e521e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to:\n",
        "#   Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "#   Tune the learning rate using GridSearchCV\n",
        "#   Print the best parameters and accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the XGBoost Classifier\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for learning rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model with GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFTejnBoyRL5",
        "outputId": "45ab4eb5-cc11-4082-991d-674729511812"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Test Accuracy: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [14:23:40] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to:\n",
        "#   Train a CatBoost Classifier\n",
        "#   Plot the confusion matrix using seaborn\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "7pA8Wpjzyn7t",
        "outputId": "23c3c8f5-e52d-4f53-8006-7d6e3a7c2d18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4178593445.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior. The dataset is imbalanced, contains missing values, and has both numeric and categorical features. Describe your step-by-step data science pipeline using boosting techniques:  Data preprocessing & handling missing/categorical values . Choice between AdaBoost, XGBoost, or CatBoost . Hyperparameter tuning strategy . Evaluation metrics you'd choose and why . How the business would benefit from your model\n",
        "\n",
        "1. Data Preprocessing & Feature Handling:-\n",
        "a) Missing Values\n",
        "\n",
        "Numerical features:\n",
        "\n",
        "Let the model handle missing values directly (XGBoost/CatBoost support this)\n",
        "\n",
        "Optionally add missing-value indicator flags for business interpretability\n",
        "\n",
        "Categorical features:\n",
        "\n",
        "Keep missing as a separate category (important risk signal)\n",
        "\n",
        "b) Categorical Variables\n",
        "\n",
        "High-cardinality features (e.g., occupation, merchant category):\n",
        "\n",
        "Prefer native categorical handling (CatBoost) or\n",
        "\n",
        "Target/ordinal encoding if using XGBoost\n",
        "\n",
        "Low-cardinality features (e.g., gender, region):\n",
        "\n",
        "No one-hot encoding to avoid sparsity\n",
        "\n",
        "c) Class Imbalance\n",
        "\n",
        "Defaults are rare ‚Üí typically 5‚Äì15%\n",
        "\n",
        "Strategies:\n",
        "\n",
        "Use class weights / scale_pos_weight\n",
        "\n",
        "Stratified train-test split\n",
        "\n",
        "Threshold tuning post-training\n",
        "\n",
        "2. Choice of Boosting Algorithm:-\n",
        "\n",
        "Final Choice: CatBoost (Primary)\n",
        "\n",
        "Why CatBoost?\n",
        "\n",
        "Handles categorical + numeric features natively\n",
        "\n",
        "Robust to missing values\n",
        "\n",
        "Uses ordered boosting ‚Üí prevents target leakage\n",
        "\n",
        "Strong performance on tabular financial data\n",
        "\n",
        "Minimal preprocessing ‚Üí lower operational risk\n",
        "\n",
        "3. Hyperparameter Tuning Strategy:-\n",
        "\n",
        "Step 1: Baseline Model\n",
        "\n",
        "Train with default parameters\n",
        "\n",
        "Validate on stratified cross-validation\n",
        "\n",
        "Step 2: Coarse Search\n",
        "\n",
        "Use RandomizedSearchCV or CatBoost‚Äôs built-in CV\n",
        "\n",
        "Key parameters:\n",
        "\n",
        "iterations\n",
        "\n",
        "depth\n",
        "\n",
        "learning_rate\n",
        "\n",
        "l2_leaf_reg\n",
        "\n",
        "class_weights\n",
        "\n",
        "Step 3: Fine Tuning\n",
        "\n",
        "Narrow search around top candidates\n",
        "\n",
        "Apply early stopping to prevent overfitting\n",
        "\n",
        "Step 4: Threshold Optimization\n",
        "\n",
        "Optimize probability cutoff for:\n",
        "\n",
        "Maximum Recall under acceptable False Positives\n",
        "\n",
        "4. Evaluation Metrics (Critical in FinTech):-\n",
        "\n",
        "Accuracy is not suitable due to imbalance.\n",
        "\n",
        "Primary Metrics\n",
        "\n",
        "ROC-AUC ‚Üí overall ranking power\n",
        "\n",
        "Recall (Default Class) ‚Üí minimize missed defaulters\n",
        "\n",
        "Secondary Metrics\n",
        "\n",
        "Precision ‚Üí cost of false rejections\n",
        "\n",
        "PR-AUC ‚Üí better for imbalanced datasets\n",
        "\n",
        "KS Statistic ‚Üí widely used in credit risk\n",
        "\n",
        "5. Model Validation & Explainability:-\n",
        "\n",
        "Use SHAP values for:\n",
        "\n",
        "Regulatory compliance\n",
        "\n",
        "Customer-level explanations\n",
        "\n",
        "Perform:\n",
        "\n",
        "Stability checks\n",
        "\n",
        "Population drift monitoring\n",
        "\n",
        "Bias & fairness analysis\n",
        "\n",
        "6. Business Impact & Value Creation:-\n",
        "üîπ Risk Reduction\n",
        "\n",
        "Fewer bad loans ‚Üí lower Non-Performing Assets (NPA)\n",
        "\n",
        "üîπ Profit Optimization\n",
        "\n",
        "Better approval decisions ‚Üí higher risk-adjusted returns\n",
        "\n",
        "üîπ Regulatory Compliance\n",
        "\n",
        "Transparent decision-making with explainable ML\n",
        "\n",
        "üîπ Operational Efficiency\n",
        "\n",
        "Automated, scalable credit scoring\n",
        "\n",
        "Faster loan approval turnaround\n",
        "\n"
      ],
      "metadata": {
        "id": "y4ikP9Acy-_R"
      }
    }
  ]
}