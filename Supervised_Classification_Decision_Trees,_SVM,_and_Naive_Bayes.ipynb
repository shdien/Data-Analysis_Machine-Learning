{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Information Gain tells us which attribute separates the data best into groups that are as pure (homogeneous) as possible.\n",
        "\n",
        "1. The Core Concept: Entropy:-\n",
        "\n",
        "To understand Information Gain, you first need to understand Entropy. Entropy is a measure of disorder or impurity in a dataset.\n",
        "\n",
        "High Entropy: The dataset is very mixed (e.g., a basket with 50% apples and 50% oranges). It is hard to predict the label of a random item.\n",
        "\n",
        "Low Entropy: The dataset is pure (e.g., a basket with 99% apples). It is easy to predict the label.\n",
        "\n",
        "2. What is Information Gain:-\n",
        "Information Gain measures the reduction in entropy achieved by splitting the dataset according to a specific attribute.\n",
        "\n",
        "Mathematically, it is the difference between the entropy of the parent node and the weighted average entropy of the child nodes.\n",
        "\n",
        "$$IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)$$\n",
        "\n",
        "3. How It Is Used in Decision Trees (ID3 Algorithm):-\n",
        "\n",
        "The decision tree algorithm (specifically ID3) uses Information Gain in a greedy approach:-\n",
        "\n",
        "Calculate Parent Entropy: It calculates the impurity of the current dataset.\n",
        "\n",
        "Test Every Attribute: It simulates splitting the data on every available feature (e.g., \"Weather\", \"Temperature\", \"Wind\").\n",
        "\n",
        "Calculate Child Entropy: For each test split, it calculates the new weighted entropy of the resulting groups.\n",
        "\n",
        "Find the Gain: It subtracts the new entropy from the parent entropy to see how much uncertainty was removed.\n",
        "\n",
        "Choose the Winner: The tree selects the attribute with the highest Information Gain to be the splitting node.\n",
        "\n",
        "Repeat: The process is repeated recursively for every branch until the nodes are pure (entropy is 0) or a stopping criterion is met.\n",
        "\n",
        "4. Limitation: Bias Towards Many Values:-\n",
        "\n",
        "One drawback of Information Gain is that it is biased towards attributes with a large number of distinct values.\n",
        "\n",
        "Example: If you had an attribute \"Date\" (which is unique for every row), splitting on it would result in perfectly pure child nodes (1 row each). The Information Gain would be maximum, but the model would just be memorizing data (overfitting) and would be useless for prediction.\n",
        "\n",
        "Solution: Advanced algorithms (like C4.5) use Gain Ratio, which penalizes attributes with too many unique branches."
      ],
      "metadata": {
        "id": "hWoB23dvNwvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "| Aspect      | Gini Impurity                       | Entropy                              |\n",
        "| ----------- | ----------------------------------- | ------------------------------------ |\n",
        "| Concept     | Misclassification probability       | Information uncertainty              |\n",
        "| Value Range | 0 to ~0.5 (binary)                  | 0 to 1 (binary)                      |\n",
        "| Computation | Faster (no logarithms)              | Slower (uses log)                    |\n",
        "| Sensitivity | Less sensitive to small changes     | More sensitive to class distribution |\n",
        "| Bias        | Favors larger, more balanced splits | Favors purer splits                  |\n",
        "| Used In     | CART                                | ID3, C4.5                            |\n",
        "| Metric Used | Gini Reduction                      | Information Gain                     |\n",
        "\n",
        "\n",
        "Behavior Comparison:-\n",
        "\n",
        "Pure node (all one class)\n",
        "\n",
        "Gini = 0\n",
        "\n",
        "Entropy = 0\n",
        "\n",
        "Maximally mixed node (50–50 binary)\n",
        "\n",
        "Gini = 0.5\n",
        "\n",
        "Entropy = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "TY_7ZhQGRqXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Pre-Pruning (Early Stopping) in Decision Trees is a technique used to stop the tree from growing further during training to prevent overfitting and improve generalization on unseen data.\n",
        "\n",
        "Why Pre-Pruning is Needed:-\n",
        "\n",
        "Prevents overfitting to training data\n",
        "\n",
        "Reduces model complexity\n",
        "\n",
        "Improves training speed\n",
        "\n",
        "Enhances performance on test data\n",
        "\n",
        "Example:-\n",
        "\n",
        "Suppose a node has 10 samples.\n",
        "If the rule says minimum samples = 15, then:\n",
        "\n",
        "No further split is allowed\n",
        "\n",
        "Node becomes a leaf\n",
        "\n",
        "Advantages:-\n",
        "\n",
        "Faster training\n",
        "\n",
        "Smaller, simpler trees\n",
        "\n",
        "Lower variance\n",
        "\n",
        "Good for large datasets\n",
        "\n",
        "Disadvantages:-\n",
        "\n",
        "May stop too early\n",
        "\n",
        "Can lead to underfitting\n",
        "\n",
        "Requires careful parameter tuning"
      ],
      "metadata": {
        "id": "YipCadMiSZOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#   Impurity as the criterion and print the feature importances (practical).\n",
        "#   Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Feature names\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Decision Tree Classifier using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Display feature importances\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pk7GeoaTFpo",
        "outputId": "b4400cd0-c6f9-44fc-d9aa-d768cc8fdd71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used primarily for classification, though it can also be used for regression.\n",
        "\n",
        "How SVM Works: The Core Concepts\n",
        "To understand SVM, you need to visualize how it draws a line between two groups of data points.\n",
        "\n",
        "The Hyperplane:-\n",
        "The hyperplane is the decision boundary that divides the data. For a binary classification (e.g., \"Spam\" vs. \"Not Spam\"), the SVM tries to place this line so that it separates the two categories as clearly as possible.\n",
        "\n",
        "\n",
        "The Margin:-\n",
        "Unlike other algorithms that might just find any line that separates the data, SVM looks for the maximum margin. The margin is the distance between the hyperplane and the closest data points from either class. A larger margin provides a \"safety buffer,\" making the model more robust and better at generalizing to new data.\n",
        "\n",
        "\n",
        "Support Vectors:-\n",
        "These are the most important data points in the set. They are the points that sit right on the edge of the margin. If you moved these specific points, the position of the hyperplane would change. They are called \"support vectors\" because they literally \"support\" or define the decision boundary.\n",
        "\n",
        "Real-World Applications:-\n",
        "\n",
        "Face Detection: Classifying parts of an image as \"face\" or \"non-face.\"\n",
        "\n",
        "Text Categorization: Sorting emails into spam/not-spam or news into categories.\n",
        "\n",
        "Bioinformatics: Classifying proteins or gene sequences.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d_NSc0raTf19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.  What is the Kernel Trick in SVM?\n",
        "\n",
        "The Kernel Trick in Support Vector Machines (SVM) is a technique that allows SVMs to handle non-linearly separable data by implicitly mapping input data into a higher-dimensional feature space, where a linear separator can be found.\n",
        "\n",
        "Mathematical Intuition:-\n",
        "\n",
        "Instead of mapping:\n",
        "\n",
        "ϕ(x):Rn→Rm\n",
        "\n",
        "SVM uses a kernel:\n",
        "\n",
        "K(xi​,xj​)=ϕ(xi​)⋅ϕ(xj​)\n",
        "\n",
        "\n",
        "| Kernel         | Formula                             | When to Use                  |\n",
        "| -------------- | ----------------------------------- | ---------------------------- |\n",
        "| Linear         | ( x_i \\cdot x_j )                   | Linearly separable data      |\n",
        "| Polynomial     | ( (x_i \\cdot x_j + c)^d )           | Feature interactions         |\n",
        "| RBF (Gaussian) | ( \\exp(-\\gamma |x_i - x_j|^2) )     | Complex, non-linear patterns |\n",
        "| Sigmoid        | ( \\tanh(\\alpha x_i \\cdot x_j + c) ) | Neural-network-like behavior |\n",
        "\n",
        "Example Intuition:-\n",
        "\n",
        "Imagine data arranged in concentric circles:\n",
        "\n",
        "Not separable in 2D\n",
        "\n",
        "After kernel transformation → separable by a hyperplane\n"
      ],
      "metadata": {
        "id": "-Pbd7WnOUItO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "#   kernels on the Wine dataset, then compare their accuracies.\n",
        "#   Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "#   on the same dataset.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Linear SVM\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Train RBF SVM\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"Linear SVM Accuracy:\", acc_linear)\n",
        "print(\"RBF SVM Accuracy:\", acc_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sDTFFLmUusu",
        "outputId": "4144ed98-2899-4024-b3d0-ccfc59c99ae4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9814814814814815\n",
            "RBF SVM Accuracy: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "It earns the name \"Naïve\" because it makes a massive, simplifying assumption: it assumes that all features are completely independent of one another.\n",
        "\n",
        "In the real world, this is almost never true. For example:\n",
        "\n",
        "In Weather: Humidity and Temperature are often related (high humidity usually accompanies certain temperatures). Naïve Bayes ignores this and treats them as unrelated signals.\n",
        "\n",
        "In Text: In the phrase \"Stock Market,\" the word \"Stock\" is highly likely to be followed by \"Market.\" Naïve Bayes treats the presence of \"Stock\" and \"Market\" as two completely independent events that just happened to occur in the same email.\n",
        "\n",
        "The Core FormulaNaïve Bayes uses the following formula to predict the class (9$y$) of a given set of features (10$X$):11$$P(y|x_1, ..., x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i|y)}{P(x_1, ..., x_n)}$$Since the denominator (the evidence) is the same for all classes being compared, the classifier simply finds the class that maximizes the numerator:$$Predicted Class = \\text{argmax } P(y) \\prod_{i=1}^{n} P(x_i|y)$$\n",
        "\n"
      ],
      "metadata": {
        "id": "W14n1qwGVGch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "1. Gaussian Naïve Bayes:-\n",
        "Best for: Continuous, real-valued data. This variant assumes that the features follow a Normal (Gaussian) Distribution (the \"Bell Curve\"). Instead of counting frequencies, it calculates the mean and standard deviation for each feature per class.\n",
        "\n",
        "Example Features: Temperature, height, weight, or blood pressure.\n",
        "\n",
        "How it works: If you’re predicting \"Gender\" based on \"Height,\" the model calculates the average height for men and women. For a new person, it checks where their height falls on those two bell curves to see which is more likely.\n",
        "\n",
        "2. Multinomial Naïve Bayes:-\n",
        "Best for: Discrete counts or frequencies. This is the most popular choice for Natural Language Processing (NLP). It assumes that features represent the number of times an event occurred.\n",
        "\n",
        "Example Features: The number of times the word \"Discount\" appears in an email, or the count of specific ingredients in a recipe.\n",
        "\n",
        "How it works: It looks at the \"Bag of Words\" (word counts). It calculates the probability of seeing a specific word frequency given a class (e.g., \"In spam emails, the word 'Prize' usually appears 3+ times\").\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "Best for: Binary/Boolean features (0 or 1). Like the Multinomial version, this is often used for text classification, but it ignores how many times a word appears. It only cares if the word is present or absent.\n",
        "\n",
        "Example Features: Does the email contain the word \"Winner\"? (Yes/No), or is a specific pixel in an image black or white?\n",
        "\n",
        "How it works: It models the data using a Bernoulli distribution. This is particularly useful for short documents where a word appearing multiple times doesn't necessarily add more \"information\" than it appearing just once\n",
        "\n"
      ],
      "metadata": {
        "id": "b9X128zlVidx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10.  Breast Cancer Dataset\n",
        "#     Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "#     dataset and evaluate accuracy.\n",
        "#     Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "#     sklearn.datasets.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Naïve Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LClSXxKYWiej",
        "outputId": "f6ed7a2b-225a-45c8-b738-48fb3bcdcabc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}